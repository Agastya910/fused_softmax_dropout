## Task: Implement Fused Softmax + Dropout Kernel (Triton)

You are an expert GPU kernel engineer. Implement a fused, numerically-stable softmax + dropout kernel in Triton and place your code into `kernels/fused_kernel.py` implementing:

def fused_softmax_dropout(x: torch.Tensor, p: float, seed: float) -> torch.Tensor

Requirements:
- Implement the kernel using OpenAI Triton.
- The softmax calculation must be numerically stable even for inputs with extreme magnitude (e.g., > 10,000).
- Dropout must be deterministic and controlled by the `seed` float.
- The kernel must handle arbitrary sequence lengths (not just powers of 2).
- Output must match PyTorch reference semantics exactly.

Note: The hardware RNG requires an integer seed. You must determine an appropriate strategy to convert the input `seed` (float) to a hardware `seed` (int) such that small changes in the float seed result in distinct dropout patterns.
